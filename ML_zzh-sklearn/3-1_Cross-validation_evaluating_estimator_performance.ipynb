{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Cross-validation:evaluating estimator performance\n",
    "在训练集上测试参数的学习效果，会得到很好的结果，因为都是在同一个数据集上。但是训练好的参数在没有见过的数据集上效果可能会不好，这就是**过拟合(over fitting)**。为了避免这种情况，应该在原始的数据集上留出一部分数据作为测试集。\n",
    "\n",
    "在*scikit-learn*中，使用*train_test_split*就可很方便地分割出测试集。\n",
    "\n",
    "下面，用iris数据集来示范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150L, 4L), (150L,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "iris=datasets.load_iris()\n",
    "iris.data.shape,iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以将数据集中的40%分成测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90L, 4L), (90L,), (60L, 4L), (60L,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(\n",
    "    iris.data,iris.target,test_size=0.4,random_state=0)\n",
    "#random_state代表随机数种子，不填的时候每次产生的随机数都不一样；填了某个数就代表选择了\n",
    "#该序号的随机数序列，每次产生的随机数都是一样的（填0每次产生的也一样）\n",
    "x_train.shape,y_train.shape,x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是这样仍然会有过拟合的问题（？没看懂）\n",
    ">there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. \n",
    "\n",
    "为了解决这个问题，需要在数据集上留出另一部分作为**验证集（validation set)**。训练集上训练结束后，在验证集上进行验证，当结果看起来成功以后，就可以在测试集上进行最终的评估。\n",
    "\n",
    "但是，将数据集分成三部分后，可以用来学习模型的数目大大减少了，因此结果可能会依赖于特定随机选取的训练集和验证集对。\n",
    "\n",
    "解决这个问题可以用**交叉验证法（cross-validation,CV)**：依然需要测试集来进行最终的评估，但是不需要验证集了。在基本的方法，**k折交叉验证（k-fold CV)**中，训练集被分为k个小的子集。\n",
    "\n",
    "大致过程如下：\n",
    "* 用k-1个子集作为训练集\n",
    "* 剩下的那个子集作为测试集\n",
    "\n",
    "k折交叉验证通常需要随机使用不同的划分重复p次，最终评估的结果是折p次k折交叉验证结果的均值，常见的有**10次10折交叉验证**。\n",
    "\n",
    "这种方法**计算消耗大**，但是**没有浪费很多数据**，在数据集很小的时候这个优点更加突出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Computing cross-validated metrics\n",
    "\n",
    "使用**cross-validation**的最简单的方法就是调用*cross_val_score*函数。\n",
    "\n",
    "下面的例子演示了如何在iris数据集上评估线性SVM模型的准确度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.96666667,  1.        ,  0.96666667,  0.96666667,  1.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf=svm.SVC(kernel='linear',C=1)\n",
    "scores=cross_val_score(clf,iris.data,iris.target,cv=5)#5-fold\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认的在每个CV中计算分数的方法是*score*，可以通过*scoring*参数来更改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.96658312,  1.        ,  0.96658312,  0.96658312,  1.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "scores=cross_val_score(\n",
    "    clf,iris.data,iris.target,cv=5,scoring='f1_macro')#以后详细阐述scoring参数含义\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当cv是整数的时候，*cross_val_score*默认使用**KFold**或者**StratifiedKFold**，后者在estimator是**ClassifierMixin**中派生而来时默认使用。\n",
    "\n",
    "同样，可以通过传输交叉验证迭代器来使用其他的交叉验证方法，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.97777778,  0.97777778,  1.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "n_samples=iris.data.shape[0]\n",
    "cv=ShuffleSplit(n_splits=3,test_size=0.3,random_state=0)\n",
    "cross_val_score(clf,iris.data,iris.target,cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如在留出的数据上测试训练好的预测器很重要，**预处理**（如归一化，特征选择等等）以及**数据变形（data transformations)**都应该在训练集中学习好然后应用到留出的数据上来进行预测。\n",
    "\n",
    "通过在cv中应用**Pipeline**让生成estimator更简单了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1.1 Obtaining predictions by cross-validation\n",
    "\n",
    "*cross_val_predict*函数和*cross_val_score*具有相似的接口，但是前者只会返回训练集中的预测结果。\n",
    "\n",
    "预测结果可以用来评估分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97333333333333338"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "predicted=cross_val_predict(clf,iris.data,iris.target,cv=10)\n",
    "metrics.accuracy_score(iris.target,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 Cross validation iterators\n",
    "\n",
    "接下来列举了根据不同的cv方法，utilities产生序号来产生分割的数据集。\n",
    "\n",
    "## 3.1.3 Cross-validation iterators for i.i.d. data\n",
    "\n",
    "假设数据是**独立同分布（Independent Identically Distributed，i.i.d.）**的，则所有的样本都来自于同样的产生过程且产生过程对之前产生的样本没有记忆。\n",
    "\n",
    "接下来的交叉验证器可以用于这种情况。\n",
    "\n",
    "**注意**\n",
    "尽管i.i.d.数据是机器学习理论中常用的假设，在实际中很少使用。如果样本是根据时间相关的进程产生的，用时间序列的cv方案<time_series_cv>更安全。\n",
    "\n",
    "### 3.1.3.1 K-fold\n",
    "\n",
    "**KFold**将所有的样本分成了相同大小的k组，称作*折（fold）*。\n",
    "\n",
    "下面是一个四个样本的2折cv例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold#返回序号\n",
    "\n",
    "X=['a','b','c','d']\n",
    "kf=KFold(n_splits=2)#分割的折数\n",
    "for train,test in kf.split(X):\n",
    "    print('%s %s'%(train,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一折都是由两个array组成的：第一个跟*训练集*有关，第二个跟*测试集*有关。这样，可以通过*numpy indexing*来产生*训练/测试集*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.],\n",
       "        [ 1.,  1.]]), array([[-1., -1.],\n",
       "        [ 2.,  2.]]), array([0, 1]), array([0, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([[0.,0.],[1.,1.],[-1.,-1.],[2.,2.]])\n",
    "y=np.array([0,1,0,1])\n",
    "X_train,X_test,y_train,y_test=X[train],X[test],y[train],y[test]\n",
    "X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3.2 Leave One Out(LOO)\n",
    "\n",
    "**留一法（LOO)**是一个简单的cv。设数据集包含m个样本，每个训练集都是从所有的样本中取m-1个构成的，剩下的一个样本作为测试集。因此，对m个样本，我们由m个不同的训练集和m个不同的测试集。这种方法没有浪费很多数据，因为只有一个样本从训练集中移除了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] [0]\n",
      "[0 2 3] [1]\n",
      "[0 1 3] [2]\n",
      "[0 1 2] [3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut#返回序号\n",
    "X=[1,2,3,4]\n",
    "loo=LeaveOneOut()\n",
    "for train,test in loo.split(X):\n",
    "    print ('%s %s' %(train,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | build models | trained on samples |\n",
    "|--------|:------------:|:------------------:|\n",
    "|K-fold |k        |(k-1)m/k|\n",
    "|LOO|m|m-1|\n",
    "\n",
    "假设两种方法中k都不是特别大，k< m ,LOO 比K折cv的计算开销更大；\n",
    "准确度方面，LOO经常导致高方差；\n",
    "如果训练集的学习曲线陡峭的话，5或者10折cv会高估泛化误差。\n",
    "\n",
    "综上，推荐使用**5或者10折cv**而不是LOO。\n",
    "\n",
    "### 3.1.3.3 Leave P Out(LPO)\n",
    "\n",
    "**LeavePOut**和**LeaveOneOut**很相似，它从所有的数据集中移除p个样本构成训练集，移除的p个样本构成测试集。对于n个样本来说，这个方法产生了（n,p)训练-测试数据对。不像LeaveOneOut和KFold,测试集在p>1的情况下会**重叠(overlap)**。\n",
    "\n",
    "下面是一个四个样本的Leave-2-Out的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[1 3] [0 2]\n",
      "[1 2] [0 3]\n",
      "[0 3] [1 2]\n",
      "[0 2] [1 3]\n",
      "[0 1] [2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeavePOut#返回序号\n",
    "X=np.ones(4)\n",
    "lpo=LeavePOut(p=2)\n",
    "for train,test in lpo.split(X):\n",
    "    print('%s %s'%(train,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3.4 Random permutations cross-validation a.k.a. Shuffle & Split\n",
    "\n",
    "**ShuffleSplit**迭代器会产生用户规定数目的独立的训练和测试集分割。样本首先被打乱然后被分成训练和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 4] [2 0]\n",
      "[1 4 3] [0 2]\n",
      "[4 0 2] [1 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit #返回的是分割数据集的序号\n",
    "X=np.arange(5)\n",
    "ss=ShuffleSplit(n_splits=3,test_size=.25,random_state=0)#n_splits 分割的次数\n",
    "for train_index,test_index in ss.split(X):\n",
    "    print('%s %s'%(train_index,test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.4 Cross-validation iterators with stratification based on class labels\n",
    "\n",
    "当目标的分类分布特别不均衡时，分类会产生问题：例如反例样本是正例样本数目的几倍。在这种情况下，推荐使用分层采样**StratifiedKFold**和**StratifiedSuffleSplit**来保证样本类别的比例相似。\n",
    "\n",
    "### 3.1.4.1 Stratified k-fold\n",
    "\n",
    "**StratifiedKFold**是*k-fold*的一个拓展，返回分层的折：每个子集包括差不多比例的目标类别。\n",
    "\n",
    "下面是一个2类轻微不平衡的10个样本的3折分层cv例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 6 7 8 9] [0 1 4 5]\n",
      "[0 1 3 4 5 8 9] [2 6 7]\n",
      "[0 1 2 4 5 6 7] [3 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold#返回序号\n",
    "X=np.ones(10)\n",
    "y=[0,0,0,0,1,1,1,1,1,1]\n",
    "skf=StratifiedKFold(n_splits=3)\n",
    "for train,test in skf.split(X,y):\n",
    "    print('%s %s'%(train,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4.2 Stratified Shuffle Split\n",
    "\n",
    "**StratifiedShuffleSplit**是*ShuffleSplit*的一个拓展，返回分层的分割集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 8 4 6 1 5] [9 7 0]\n",
      "[1 7 8 9 6 0 3] [4 2 5]\n",
      "[9 0 5 6 2 1 8] [7 4 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "X=np.ones(10)\n",
    "y=[0,0,0,0,1,1,1,1,1,1]\n",
    "sss=StratifiedShuffleSplit(n_splits=3,test_size=.25,random_state=0)\n",
    "for train,test in sss.split(X,y):\n",
    "    print('%s %s'%(train,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.5 Cross-validation iterators for grouped data\n",
    "\n",
    "若潜在的产生过程产生了相关的样本组，则i.i.d.假设不成立。\n",
    "\n",
    "这种数据组是区域特定的。若从很多患者处采集了药物数据，每个患者处都采集了很多样本。这种数据对每个人是相关的。在这个例子中，每个样本的患者序号就是组别。\n",
    "\n",
    "这种情况下我们想知道特定组中训练好的模型是否能在未见过的组别中泛化很好。我们需要保证验证折中的所有样本的来自组别都没有在对应的训练折中出现。\n",
    "\n",
    "接下来的cv分裂器可以实现这个。样本的组别通过*groups*这个变量来指定。\n",
    "\n",
    "### 3.1.5.1 Group k-fold\n",
    "\n",
    "**GroupKFold**是*k-fold*的一个拓展，保证了同一个组没有在成对的测试集和训练集中出现。例如：数据从不同的目标中采集，每个目标都采集了很多样本；如果模型足够灵活从高度个人特定的特征中学习，在新的目标中泛化就可能失败。**GroupKFold**能检测到这种过拟合的情况。\n",
    "\n",
    "假设你有3个目标，每个目标存在相关联的数字1-3："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [6 7 8 9]\n",
      "[6 7 8 9] [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold#返回序号\n",
    "\n",
    "X=[0.1,0.2,2.2,2.4,2.3,4.55,5.8,8.8,9,10]\n",
    "y=['a','b','b','b','c','c','c','d','d','d']\n",
    "groups=[1,1,1,2,2,2,3,3,3,3]\n",
    "gkf=GroupKFold(n_splits=2) #n_splits 不能大于组别数目\n",
    "for train,test in gkf.split(X,y,groups=groups):\n",
    "    print('%s %s'%(train,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个目标都在不同的测试折中，相同的目标不会在测试集和训练集中出现。折没有相同的大小，因为数据的均衡。\n",
    "\n",
    "### Leave One Group Out\n",
    "\n",
    "**LeaveOneGroupOut**使一种cv方案，根据第三方指定的整数数组留出样本。\n",
    "\n",
    "每个训练集都由除了一个与特定组相关的样本外的样本组成。\n",
    "\n",
    "如：在多次实验的情况下，**LeaveOneGroupOut**可以基于不同的实验来产生cv：用除了一个实验的样本外的其他样本来产生训练集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6] [0 1]\n",
      "[0 1 4 5 6] [2 3]\n",
      "[0 1 2 3] [4 5 6]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut#返回序号\n",
    "X=[1,5,10,50,60,70,80]\n",
    "y=[0,1,1,2,2,2,2]\n",
    "groups=[1,1,2,2,3,3,3]\n",
    "logo=LeaveOneGroupOut()\n",
    "for train,test in logo.split(X,y,groups=groups):\n",
    "    print ('%s %s'%(train,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Group k-fold 与 Leave One Group Out 区别**\n",
    "* Group k-fold 指定k（组）折，可能留出几个组\n",
    "* Leave One Group Out中只可能留出一个组\n",
    "\n",
    "### 3.1.5.3 Leave P Groups Out\n",
    "\n",
    "**LeavePGroupsOut**和**LeaveOneGroupOut**相似，但是移除了与*P*个组相关的样本。\n",
    "\n",
    "下面是一个*Leave-2-Group Out*的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5] [0 1 2 3]\n",
      "[2 3] [0 1 4 5]\n",
      "[0 1] [2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeavePGroupsOut#返回序号\n",
    "\n",
    "X=np.arange(6)\n",
    "y=[1,1,1,2,2,2]\n",
    "groups=[1,1,2,2,3,3]\n",
    "lpgo=LeavePGroupsOut(n_groups=2)\n",
    "for train,test in lpgo.split(X,y,groups=groups):\n",
    "    print('%s %s'%(train,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5.4 Group Shuffle Split\n",
    "\n",
    "**GroupShuffleSplit**是**ShuffleSplit**和**LeavePGroupsOut**的结合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 6 7] [4 5]\n",
      "[2 3 4 5 6 7] [0 1]\n",
      "[0 1 2 3 4 5] [6 7]\n",
      "[0 1 4 5 6 7] [2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit#返回序号\n",
    "\n",
    "X=[0.1,0.2,2.2,2.4,2.3,4.55,5.8,0.001]\n",
    "y=['a','b','b','b','c','c','c','a']\n",
    "groups=[1,1,2,2,3,3,4,4]\n",
    "gss=GroupShuffleSplit(n_splits=4,test_size=0.25,random_state=0)\n",
    "for train,test in gss.split(X,y,groups=groups):\n",
    "    print('%s %s'%(train,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方法在需要LeavePGroupsOut，但组别的数量太大了导致产生所有可能的P个组别的代价太大了时很有用。\n",
    "\n",
    "## 3.1.6 Predefined Fold-Splits/Validation-Sets\n",
    "\n",
    "在一些数据集中，预先定义好的分割训练集和验证集的方法已经存在了。使用**PredifinedSplit**可以使用这些折如寻找超参数时。\n",
    "\n",
    "例如，使用验证集时，将验证集中所有样本的*test_fold*设为0，其他的样本中设为1。\n",
    "\n",
    "## 3.1.7 Cross validation of time series data\n",
    "\n",
    "时间序列的特征是由在时间上相近的观察之间的相关性所描述的。但是，传统的cv方法如*kfold*和*shufflesplit*假设样本是**独立同分布**的，会导致时间序列数据中训练和测试样本的不知原因的相关性。因此，在与训练模型的时间序列最不相似的*将来*观察上评估模型很重要。为了解决这个问题，可以使用**TimeSeriesSplit**。\n",
    "\n",
    "### 3.1.7.1 Time Series Split\n",
    "\n",
    "**TimeSeriesSplit**是*k-fold*的一个拓展，第一次返回前k折作为训练集，第k+1折作为测试集。接下来的训练把之前测试集中所有数据加到第一个训练部分中，通常用于训练模型。\n",
    "\n",
    "这种方法可以用来交叉验证特定时间间隔观察到的时间序列样本。\n",
    "\n",
    "下面是一个6个样本的3折时间序列cv例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] [3]\n",
      "[0 1 2 3] [4]\n",
      "[0 1 2 3 4] [5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit#返回序号\n",
    "\n",
    "X=np.array([[1,2],[3,4],[1,2],[3,4],[1,2],[3,4]])\n",
    "y=np.array([1,2,3,4,5,6])\n",
    "tscv=TimeSeriesSplit(n_splits=3)\n",
    "for train,test in tscv.split(X):\n",
    "    print('%s %s'%(train,test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
